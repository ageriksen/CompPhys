\documentclass[10pt, twocolumn]{revtex4-1}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage[colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{color}

%\begin{figure}[hbtp]
%\includegraphics[scale=0.4]{test1.pdf}
%\caption{Exact and numerial solutions for $n=10$ mesh points.} 
%\label{fig:n10points}
%\end{figure}

\begin{document}
\title{Project 1}
\author{Anders Eriksen}
\begin{abstract}
Attempting a sleeker version of the general solver of a tridiagonal matrix reducing the general case from ~8n FLOPs to ~4n. "n" here being the dimensionality of the matrix
\end{abstract}
\maketitle

\section{Introduction}
This is a rushed report on an unfinished experiment. Repeated issues with programming originating from both the OS and unfamiliarity with the language eventually led to capitulation and falling back on python to attempt to finish in time. I have attempted to organize the scattered results and discussing them. I have also attempted to glean personal developement in the experiment despite the lackluster results. 

\section{Theory, algorithms and methods}
We begin with the simple differential equation 
\begin{equation*}
-u''(x) = f(x).
\end{equation*}
which we can discretize as 
\begin{equation*}
   -\frac{v_{i+1}+v_{i-1}-2v_i}{h^2} = f_i  \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=1,\dot    s, n,
\end{equation*}
Where the indexes signify preveious, current or next point in our function u. $f_i$ is the disretized $f(x_i) = f_i$. "h" is the stepsize and is proportional to the number of mesh-points n. Our function comes with the conditions that $x \in [0,1]$ and $u(0)=u(1)=0$. An even stepsize then should be $h = \frac{x_{max}- x_{min}}{n} = \frac{1}{n}$. Luckily for our purposes, u has a closed form sollution 
\begin{equation*}
	u(x) = 1-(1-e^{-10})x-e^{-10x}
\end{equation*}
Staring at the discretization for a spell, you can see that the equation can be rewritten as a matrix multiplication of a vector $\vec{u}$: $-\hat{A}\vec{u}=\vec{d}$, with $d_i = h^2\cdot f_i$.  Here, the matrix $\hat{A}$ should be a tri-diagonal matrix of values $a_{11} = b_1 = 2$ along the diagonal and $a_{ii_{\pm 1}} = -1$ to either side. Writing this out index for index makes

 \[
\mathbf{A} = \begin{bmatrix}
                    2& -1& 0 &\dots   & \dots &0 \\
                    -1 & 2 & -1 &0 &\dots &\dots \\
                    0&-1 &2 & -1 & 0 & \dots \\
                    & \dots   & \dots &\dots   &\dots & \dots \\
                    0&\dots   &  &-1 &2& -1 \\
                    0&\dots    &  & 0  &-1 & 2 \\
              \end{bmatrix}
              \begin{bmatrix}
              		u_1 \\
              		u_2 \\
              		\dots \\
              		\dots \\
              		\dots \\
              		u_n
              \end{bmatrix} =
			  \begin{bmatrix}
              		d_1 \\
              		d_2 \\
              		\dots \\
              		\dots \\
              		\dots \\
              		d_n
              \end{bmatrix}              
\]
From this, we can make an upper triangular matrix through the Gaussian row reduction going both forward and backward to solve for $\vec{u}$, resulting in an algorithm (once properly optimized) of, first with forward substitution and the backwards, after specifically finding the 1st object 
\begin{equation*}
	\begin{aligned}
		e &= \frac{a_{i-1}}{b_{i-1}} \\
		\tilde{b_i} &= b_i - r\cdot c_{i-1} \\
		\tilde{d_i} &= d_i - r\cdot\tilde{d_{i-1}} \\ \\
		u_n &= \frac{\tilde{d_n}}{\tilde{b_n}} \\
		u_i &= \frac{(\tilde{d_i} - c_i\cdot u_{i+1})}{\tilde{b_i}}
	\end{aligned}
\end{equation*}
This algorithm has a total of $(8n-13)$ FLOPs, which approximates to 8n for large n. This is for the general case, however. In our special case, we can reduse the number of FLOPs quite a bit. By half, in fact. 
Knowing $a_i = c_{i} = -1$ and $b=2$, $e = -\frac{1}{2}$. And this is a point I'm uncertain of. From the previous algorithm, to my mind, $\tilde{b_{i}} = b_{i} - r \cdot c_{i-1}$ should become $\tilde{b_i}=2-\frac{1}{2}=\frac{3}{2}$. This, however is different from an algorithm I found on line 47 of the code examples for project1 2018 on the github page. Here, b is given as $\tilde{b_i} = \frac{(i+1)}{i}$, note that my b is this script's d. The 2 produce different values, but neither is really a good fit. Therefore, though I believe I have managed to halve the number of FLOPs I cannot say the algorithm is the same. see figures 1 and 2 for illustrations of the latests fits.

\begin{figure}[hbtp]
\includegraphics[scale=0.4]{../specificNumAna_b-halfN10.png}
\caption{specific algorithm solution for u at n = 10 mesh points with $b=\frac{3}{2}$} 
\end{figure}

\begin{figure}[hbtp]
\includegraphics[scale=0.4]{../specificNumAna_bip1overiN10.png}
\caption{specific algorithm solution for u at n = 10 mesh points with $b=\frac{(i+1)}{i}$} 
\end{figure}
This was, unfortunately as far along as I got. The plan further was to compare the CPU time spent on the 2 algorithms for n up to $10^6$ and from there, the base 10 logarithm of the relative error between the exact and the numerical solutions. This point is to locate the point in which the algortihmic error is overtaken by the loss of precision from the computations. Finally, the plan was to perform a different approximation of the solution through LU decomposition rather than the Gaussian row reductions. 

\section{Results and discussions}
For the general tridiagonal fit, the numerical quickly approaches the numerical solution, increasing steadily up to 10 000 mesh points. For comparison, see figures 3 and 4.
\begin{figure}[hbtp]
\includegraphics[scale=0.4]{../NumAna10.png}
\caption{General numeric versus exact solution at n=10 mesh points} 
\end{figure}

\begin{figure}[hbtp]
\includegraphics[scale=0.4]{../NumAna10000_wide.png}
\caption{general numerical versus exact solution at n=10 000 mesh points.} 
\end{figure}
For the amount of mesh points at this point, there is little delay in running the algorithm. Rather the import and initialization phase outstrips even the last solution with 10 000 points. Because the simulation was only up to 10 000 points, loss of precision from numerical operations could not quite come into play. There is also little to contrast with besides the various runs of n, in which we see a far better fitt with higher n, though this is somewhat trivial. 

\section{conclusions}
There is little to conclude, as there was no true questions raised. The experiment flatlined before it could even get properly off the ground. One can always find numerous reasons, but I believe the biggest ones was initial issues with the OS. These in turn led to awkward transition into the c++ language once I transfered to a unix system. this caused major delays in the schedule, and I never managed to catch up. I had to switch to python in a last ditch effort to get it done by the deadline, but this was not to be, as I slowly ran out of time wresteling with various issues along the implementation of the algorithm. I am somewhat satisfied with the overall layout of the script, however, as the algorithm was in a separate module, which allowed for easier reading and editing without fear of cutting off a function call somewhere. Furthermore, I have previously had trouble implementing classes and modules into my programming. In this instance, I have gained new ground. All in all, though I have learned some in regards to implementations of algorithms and creating a modular structure, there were a lot of kinks I could not quite solve. 

\section{addendum}
this is a link to the git repo:
https://github.com/andersgeriksen/CompPhys/tree/master/Project1

\end{document}